{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import os\n",
    "import torch\n",
    "os.chdir(os.path.join(os.getcwd(), '..', 'maverick-coref-main'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ast\n",
    "from maverick import Maverick\n",
    "\n",
    "# Configuration\n",
    "DATAFRAME_FOLDER_PATH = os.makedirs(\"../data/Dataframe/\", exist_ok=True)\n",
    "DATAFRAME_FOLDER_PATH = '../data/Dataframe/'\n",
    "LITBANK_CASE_FOLDER_PATH = os.makedirs(\"../data/Litbank_Case/\", exist_ok=True)\n",
    "LITBANK_CASE_FOLDER_PATH = \"../data/Litbank_Case/\"\n",
    "\n",
    "# Data\n",
    "LitBank_train=[]\n",
    "with jsonlines.open(\"../data/litbank/train.english.jsonlines\") as read_file:\n",
    "    for line in read_file.iter():\n",
    "        LitBank_train.append(line)\n",
    "        \n",
    "PreCo_train=[]\n",
    "with jsonlines.open(\"../data/preco/train.jsonl\") as read_file:\n",
    "    for line in read_file.iter():\n",
    "        PreCo_train.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>sentences</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217_sons_and_lovers_brat_0</td>\n",
       "      <td>[[PART, ONE, CHAPTER, I, THE, EARLY, MARRIED, ...</td>\n",
       "      <td>[[[9, 10], [900, 900], [905, 905]], [[744, 744...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805_this_side_of_paradise_brat_0</td>\n",
       "      <td>[[BOOK, ONE, --, The, Romantic, Egotist, CHAPT...</td>\n",
       "      <td>[[[3, 5]], [[460, 460], [653, 653], [1906, 190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1695_the_man_who_was_thursday_a_nightmare_brat_0</td>\n",
       "      <td>[[CHAPTER, I, .], [THE, TWO, POETS, OF, SAFFRO...</td>\n",
       "      <td>[[[3, 5]], [[182, 183], [406, 406], [103, 103]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_key  \\\n",
       "0                        217_sons_and_lovers_brat_0   \n",
       "1                  805_this_side_of_paradise_brat_0   \n",
       "2  1695_the_man_who_was_thursday_a_nightmare_brat_0   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [[PART, ONE, CHAPTER, I, THE, EARLY, MARRIED, ...   \n",
       "1  [[BOOK, ONE, --, The, Romantic, Egotist, CHAPT...   \n",
       "2  [[CHAPTER, I, .], [THE, TWO, POETS, OF, SAFFRO...   \n",
       "\n",
       "                                            clusters  \n",
       "0  [[[9, 10], [900, 900], [905, 905]], [[744, 744...  \n",
       "1  [[[3, 5]], [[460, 460], [653, 653], [1906, 190...  \n",
       "2  [[[3, 5]], [[182, 183], [406, 406], [103, 103]...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_LBtrain = pd.DataFrame(LitBank_train)[['doc_key', 'sentences', 'clusters']]\n",
    "# df_LBtrain.to_csv(DATAFRAME_FOLDER_PATH+'df_LBtrain.csv', index=False)\n",
    "df_LBtrain = pd.read_csv(DATAFRAME_FOLDER_PATH+'df_LitBank_train.csv')\n",
    "df_LBtrain['sentences'] = [ast.literal_eval(data) for data in df_LBtrain['sentences']]\n",
    "df_LBtrain['clusters'] = [ast.literal_eval(data) for data in df_LBtrain['clusters']]\n",
    "df_LBtrain.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_offsets(ontonotes_sentence, coreference_offsets):\n",
    "    \"\"\"\n",
    "    주어진 coreference offset이 포함된 문장만 추출하여 순서대로 정리하고,\n",
    "    문장이 일부만 추출되었기 때문에 offset 값을 새로운 기준에 맞게 조정한다.\n",
    "    \n",
    "    Args:\n",
    "        ontonotes_sentence (list of lists): 전체 문장 리스트 (각 문장은 단어 리스트)\n",
    "        coreference_offsets (list of lists): coreference 표현의 시작/끝 단어 인덱스 쌍\n",
    "\n",
    "    Returns:\n",
    "        tuple: 추출된 문장 리스트, 새롭게 조정된 offset 리스트\n",
    "    \"\"\"\n",
    "    unique_sentences = []\n",
    "    adjusted_offsets = []\n",
    "    \n",
    "    # 누적 길이를 추적하여 새 offset 계산에 활용\n",
    "    cumulative_length = 0\n",
    "    previous_length = 0\n",
    "\n",
    "    for offset in coreference_offsets:\n",
    "        for sentence_idx, sentence in enumerate(ontonotes_sentence):\n",
    "            sentence_start = sum(len(s) for s in ontonotes_sentence[:sentence_idx])\n",
    "            sentence_end = sentence_start + len(sentence) - 1\n",
    "\n",
    "            # 현재 offset이 해당 문장 범위에 포함되는지 확인\n",
    "            if sentence_start <= offset[0] <= sentence_end:\n",
    "                if sentence not in unique_sentences:\n",
    "                    unique_sentences.append(sentence)\n",
    "                    cumulative_length += previous_length  # 앞선 문장 길이 누적\n",
    "                # 새로운 offset 계산\n",
    "                new_offset_start = offset[0] - sentence_start\n",
    "                new_offset_end = new_offset_start + (offset[1] - offset[0])\n",
    "                adjusted_offset = [\n",
    "                    new_offset_start + cumulative_length,\n",
    "                    new_offset_end + cumulative_length\n",
    "                ]\n",
    "                adjusted_offsets.append(adjusted_offset)\n",
    "                previous_length = len(unique_sentences[-1])\n",
    "                break\n",
    "\n",
    "    return unique_sentences, adjusted_offsets\n",
    "\n",
    "def find_coreference_terms(ontonotes_format, clusters_token_offsets):\n",
    "    \"\"\"\n",
    "    주어진 offset으로부터 coreference cluster에 해당하는 실제 단어들을 추출한다.\n",
    "\n",
    "    Args:\n",
    "        ontonotes_format (list of list): 전체 문장 구조 (단어 리스트의 리스트)\n",
    "        clusters_token_offsets (list of lists): 각 cluster에 속하는 단어 범위(offset)\n",
    "\n",
    "    Returns:\n",
    "        list: 각 cluster별로 단어 그룹을 저장한 리스트\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sentence_lengths = [len(sentence) for sentence in ontonotes_format]  # 문장별 길이\n",
    "\n",
    "    for cluster in clusters_token_offsets:\n",
    "        if len(cluster) == 1:\n",
    "            continue  # 하나만 있으면 coref로 볼 수 없으므로 skip\n",
    "\n",
    "        cluster_terms = []\n",
    "        for start, end in cluster:\n",
    "            sentence_idx = 0\n",
    "            for i, length in enumerate(sentence_lengths):\n",
    "                if start < length:\n",
    "                    sentence_idx = i\n",
    "                    break\n",
    "                start -= length\n",
    "                end -= length\n",
    "            # 해당 문장에서 단어 추출\n",
    "            terms = ontonotes_format[sentence_idx][start:end+1]\n",
    "            cluster_terms.append(terms)\n",
    "        result.append(cluster_terms)\n",
    "\n",
    "    return result\n",
    "\n",
    "def sort_by_first_value(data):\n",
    "    \"\"\"\n",
    "    주어진 리스트를 각 원소의 첫 번째 값 기준으로 정렬한다.\n",
    "    예: [[5, 6], [1, 2]] -> [[1, 2], [5, 6]]\n",
    "    \"\"\"\n",
    "    return sorted(data, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df_LBtrain)):  # LitBank train 데이터셋에서 각 케이스(index)에 대해 반복\n",
    "    coref_list = []\n",
    "    litbank_case = pd.DataFrame()\n",
    "\n",
    "    # 문자열 형태로 저장된 데이터를 실제 리스트로 변환\n",
    "    for col in litbank_case.columns:\n",
    "        litbank_case[col] = litbank_case[col].apply(ast.literal_eval)\n",
    "\n",
    "    # Coreference cluster 추출\n",
    "    for data in df_LBtrain['clusters'][index]:\n",
    "        coref = data\n",
    "        if len(coref) == 1:\n",
    "            continue  # 하나만 있는 경우는 생략\n",
    "        coref_list.append(coref)\n",
    "        \n",
    "    litbank_case['coref'] = coref_list\n",
    "\n",
    "    # 문장과 offset 추출 및 재조정\n",
    "    extracted_sentence_list, adjusted_offsets_list = [], []\n",
    "    for data in litbank_case['coref']:\n",
    "        extracted_sentence, adjusted_offsets = calculate_new_offsets(df_LBtrain['sentences'][index], data)\n",
    "        extracted_sentence_list.append(extracted_sentence)\n",
    "        adjusted_offsets_list.append(adjusted_offsets)\n",
    "        \n",
    "    litbank_case['extracted_sentence'] = extracted_sentence_list\n",
    "    litbank_case['adjusted_offsets'] = adjusted_offsets_list\n",
    "\n",
    "    # offset 정렬\n",
    "    litbank_case['adjusted_offsets'] = litbank_case['adjusted_offsets'].apply(lambda x: sort_by_first_value(x))\n",
    "\n",
    "    # 실제 단어 표현 추출\n",
    "    litbank_case['text'] = find_coreference_terms(df_LBtrain['sentences'][index], litbank_case['coref'])\n",
    "\n",
    "    # CSV 파일로 저장 (index에 따라 파일명 구분)\n",
    "    litbank_case.to_csv(f'{LITBANK_CASE_FOLDER_PATH}litbank_case_{index}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coref</th>\n",
       "      <th>extracted_sentence</th>\n",
       "      <th>adjusted_offsets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[9, 10], [900, 900], [905, 905]]</td>\n",
       "      <td>[['PART', 'ONE', 'CHAPTER', 'I', 'THE', 'EARLY...</td>\n",
       "      <td>[[9, 10], [22, 22], [27, 27]]</td>\n",
       "      <td>[['THE', 'MORELS'], ['They'], ['their']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[744, 744], [725, 733], [874, 875], [680, 681...</td>\n",
       "      <td>[['Mrs.', 'Morel', 'was', 'not', 'anxious', 't...</td>\n",
       "      <td>[[27, 27], [8, 16], [52, 53], [64, 65], [113, ...</td>\n",
       "      <td>[['it'], ['the', 'Bottoms', ',', 'which', 'was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[450, 454], [18, 19], [22, 23], [247, 250], [...</td>\n",
       "      <td>[['To', 'accommodate', 'the', 'regiments', 'of...</td>\n",
       "      <td>[[35, 39], [64, 65], [68, 69], [90, 93], [116,...</td>\n",
       "      <td>[['the', 'site', 'of', 'Hell', 'Row'], ['Hell'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               coref  \\\n",
       "0                  [[9, 10], [900, 900], [905, 905]]   \n",
       "1  [[744, 744], [725, 733], [874, 875], [680, 681...   \n",
       "2  [[450, 454], [18, 19], [22, 23], [247, 250], [...   \n",
       "\n",
       "                                  extracted_sentence  \\\n",
       "0  [['PART', 'ONE', 'CHAPTER', 'I', 'THE', 'EARLY...   \n",
       "1  [['Mrs.', 'Morel', 'was', 'not', 'anxious', 't...   \n",
       "2  [['To', 'accommodate', 'the', 'regiments', 'of...   \n",
       "\n",
       "                                    adjusted_offsets  \\\n",
       "0                      [[9, 10], [22, 22], [27, 27]]   \n",
       "1  [[27, 27], [8, 16], [52, 53], [64, 65], [113, ...   \n",
       "2  [[35, 39], [64, 65], [68, 69], [90, 93], [116,...   \n",
       "\n",
       "                                                text  \n",
       "0           [['THE', 'MORELS'], ['They'], ['their']]  \n",
       "1  [['it'], ['the', 'Bottoms', ',', 'which', 'was...  \n",
       "2  [['the', 'site', 'of', 'Hell', 'Row'], ['Hell'...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litbank_case_0 = pd.read_csv(\"../data/Litbank_Case/litbank_case_0.csv\")\n",
    "litbank_case_0.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maverick_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
